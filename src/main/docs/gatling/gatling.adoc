= Torturing Wildflies with Gatling Stress Tool

== Introduction

This document will describe how to perform *performance* and *load* tests using http://gatling.io[*Gatling*^] stress tool .

== Objectives

* Introduce Gatling tool
* Setup Gatling in our https://github.com/rmpestano/cdi-crud[CDI Crud application^];
* Show the difference between performance and load testing
* Example on how to create a simulation;
** example of rest api calls
** example of JSF web application calls
* Execute performance tests after some system's improvements;
* Torturing Wildfly application server through load/stress testing.

== A note on Performance x Stress testing
For the purpose of this article I will use the definitions below to differentiate these two kinds of test:

* *Performance test*: This kind of test aims to constantly check if the system's performance meets some pre defined expectations like eg: response time must not be greater then 1 second.
Note that this kind of test is *not* destructive and should be able to run more frequently than a load/stress test. It is not interest in finding defects.
* *Stress test*: is meant to test the system by constantly and steadily increasing the load on the system until it reaches a threshold limit.
The goal of load testing is to expose defects (eg: memory leaks) and/or observe resources consumption in real or even abnormal circumstances like very high traffic.

One example of bug that a load/stress test can discover is the infamous http://stackoverflow.com/questions/10176945/java-using-jdbc-too-many-connections[too many connections open].



== Why Gatling?
The main advantage of *Gatling*, in my opinion, is that it is intuitive. When you write performance tests with the gatling API you're really describing user steps
so it is easy to *understand*, *write* and *maintain* the test.

For example, when using JMeter you hardly will write a simulation without the help of its GUI to record the test. Also when a change is done in the simulation it is hard to know what was changed because its all XML.


Another advantage is that gatling is based on http://doc.akka.io/docs/akka/snapshot/scala/actors.html[actors model], http://netty.io/[Netty] and en.wikipedia.org/wiki/Non-blocking_I/O_(Java)[NIO]. So what that means?

* it is asynchronous - based on netty async client;
* non blocking I/O - no need to wait for responses;
* each user runs on its thread;
* probably perform better then other stress tools, see this benchmark on https://flood.io/blog/13-stress-testing-jmeter-and-gatling[Gatling versus JMeter^];
* produces more reliable simulation (it's more approximated to real application behaviour - mainly due to its threading model)

Also IDE support is a big advantage

image::ide-support.png[align="center"]

And finally, http://gatling.io/#/docs[the documentation^] is really great. Also the http://gatling.io/docs/2.1.6/cheat-sheet.html[_Cheat sheet_^] is really helpful.

== Setup Gatling in our project

To configure the tool on our sample project we basically will use the gatling maven plugin as follows:

.pom.xml
----
<plugin>
	<groupId>io.gatling</groupId>
	<artifactId>gatling-maven-plugin</artifactId>
	<version>2.1.5</version>
	<configuration>
		<dataFolder>src/test/resources/data</dataFolder> <1>
	</configuration>
</plugin>
----

<1> External data (samples, user logins etc...) to be used in the simulation.

Also include the following maven dependency:

.pom.xml
----
<dependency>
	<groupId>io.gatling.highcharts</groupId>
	<artifactId>gatling-charts-highcharts</artifactId>
	<version>2.1.5</version>
	<scope>test</scope>
</dependency>
----

== Anatomy of a gatling test

* *Simulation* footnote:[_Simulation_ is the name which is usually given to performance tests because they try to simulate the application's
                     usage under real or even abnormal circumstances like e.g putting/simulating a lot of users using the app at the same time.]: Is the load/scalability/stress/performance test itself. It is made of multiple _scenarios_;
* *Scenario*: is the path a user takes to complete an action, a typical user behaviour. It is basically composed by http (it supports other protocols like web sockets) requests;
* *Setup*: it is the configuration of our simulation. It will tell how much users or throughput each scenario will have. A setup is composed by a set of steps which will configure how users/throughput will be _injected_.
* *Assertions*: It is the expectations of our simulation, assertions will define if our test was successful or not. They can be declared at request level or globally.

Technically saying, a Gatling simulation is basically a scala file using an specific DSL which will perform calls (usually http) to application under test.

Let's see an example that shows a Gatling simulation.

=== A simple example

Here is a simple simulation which makes http call to our application REST API:

[source, scala]
----
import io.gatling.core.Predef._
import io.gatling.http.Predef._
import scala.concurrent.duration._

class CdiCrudRestSimulation extends Simulation{


  val httpProtocol = http <1>
    .baseURL("http://localhost:8080/cdi-crud/")
    .acceptHeader("application/json;charset=utf-8")
    .contentTypeHeader("application/json; charset=UTF-8")

  val listCarsRequest = http("list cars") <2>
    .get("rest/cars/")
    .check(status.is(200)) <3>

  val listCarsScenario = scenario("List cars") //<4> A scenario is a group of one or more requests
    .exec(listCarsRequest)

  setUp( //<4> scenario setup
      listCarsScenario.inject(
        atOnceUsers(10),  <5>
        rampUsersPerSec(1) to (10) during(20 seconds),  <6>
        constantUsersPerSec(2) during (15 seconds))
       )
      .protocols(httpProtocol)  <7>
      .assertions( <8>
        global.successfulRequests.percent.greaterThan(95), // for all requests
        details("list cars").responseTime.mean.lessThan(50), // for specific group of requests
        details("list cars").responseTime.max.lessThan(300)
      )

}
----

<1> Template for all http requests;
<2> Stores this request in a local variable;
<3> Request assertion;
<4> Scenarios configuration
<5> Add 5 users at the same time (each on its on thread). They will fire one request (wait its response) each one.
<6> scale from 1 to 10 users during 20 seconds (one user is added on each 2 seconds. On the last second of this step 10 users will fire requests simultaneously)
<7> 2 users per second during 15 seconds (i fell quite dummy explaining this because the DSL is really *self explanatory*)
<8> this section makes assertions on all or a group of requests

NOTE: I've already talked about the https://rpestano.wordpress.com/2014/12/21/some-words-on-javaee-rest-and-swagger/[REST API under test here^]


This simulation fires a total of 150 request in 34 seconds, here is the console output:

----
================================================================================
---- Global Information --------------------------------------------------------
> request count                                        150 (OK=150    KO=0     )
> min response time                                      8 (OK=8      KO=-     )
> max response time                                     38 (OK=38     KO=-     )
> mean response time                                    21 (OK=21     KO=-     )
> std deviation                                          5 (OK=5      KO=-     )
> response time 50th percentile                         22 (OK=22     KO=-     )
> response time 75th percentile                         24 (OK=24     KO=-     )
> mean requests/sec                                  4.343 (OK=4.343  KO=-     )
---- Response Time Distribution ------------------------------------------------
> t < 800 ms                                           150 (100%)
> 800 ms < t < 1200 ms                                   0 (  0%)
> t > 1200 ms                                            0 (  0%)
> failed                                                 0 (  0%)
================================================================================

Reports generated in 0s.
Please open the following file: /home/pestano/projects/cdi-crud/target/gatling/results/CdiCrudRestSimulation-1430707109729/index.html
Global: percentage of successful requests is greater than 95 : true
list cars: mean of response time is less than 50 : true
list cars: max of response time is less than 300 : true
----

So here are the steps execution order:

. *atOnceUsers(10)*: 10 users execute the scenario at the same time and "go away". As the scenario has only one http rest (list cars) we end up with 10 request till now;
. *rampUsersPerSec(1) to (10) during(20 seconds)*: in this next step the simulation adds 1 users on every 2 seconds during 20 seconds. From _moment_ 1 sec to 2 sec the first user fires two requests then another user is added.
from _moment_ 2 sec to 4 sec these 2 users fires 4 request, from second 4 to 6 we have 3 users (3 req per sec x 2 sec = 6) which leads to 6 req and so on, this is actually an http://en.wikipedia.org/wiki/Arithmetic_progression[arithmetic progression]. At the end of this step we have more 110 requests
. *constantUsersPerSec(2) during (15 seconds)*: here we have 2 requests per second (because we have only one request on this scenario x 2 users) during 15 seconds = 30 req.

So at the end of the 3 steps we have 150 requests.

And here are some graphical reports generated by Gatling which illustrates this execution:

|====

| image:simple-simulation01.png[400,300,link="https://www.flickr.com/photos/131177342@N02/17155121117/",window="_blank"] | image:simple-simulation03.png[400,300,,link="https://www.flickr.com/photos/131177342@N02/17175013220/",window="_blank"]

| image:simple-simulation02.png[400,300,link="https://www.flickr.com/photos/131177342@N02/16740080194/",window="_blank"] | image:simple-simulation04.png[400,300,link="https://www.flickr.com/photos/131177342@N02/16742318423/",window="_blank"]

| image:simple-simulation-detail01.png[400,300,link="https://www.flickr.com/photos/131177342@N02/17362231561/",window="_blank"] | image:simple-simulation03.png[400,300,link="https://www.flickr.com/photos/131177342@N02/17175012290/",window="_blank"]

| image:simple-simulation-detail02.png[400,300,link="https://www.flickr.com/photos/131177342@N02/17176369579/",window="_blank"] | image:simple-simulation04.png[400,300,link="https://www.flickr.com/photos/131177342@N02/17175011950/",window="_blank"]

|====

== The simulation

The resulting simulation of this document can be https://github.com/rmpestano/cdi-crud/blob/master/src/test/scala/com/cdi/crud/perf/CdiCrudRestSimulation.scala[found here].

=== Find users request

TODO show feeder

=== Add users request

TODO comment on ELFileBody

=== Web request example

TODO show the recorder

== Torturing Wildflies
Now the section that entitles this post, *Wildflies* is meant to be the plural of http://wildfly.org[WildflyAS] which will be the target of our simulation.

IMPORTANT: I will run the simulation and the application (the one deployed on Wildfly) on the same machine. This is *NOT ideal* cause both will compete for resources (CPU and memory) but it is simpler to show the concepts.

=== Software and hardware

* Ubuntu 14.04/amd64;
* Java 8u40;
* Wildfly 9.0.0CR1;
* The application under test uses JavaEE7 stack, more https://github.com/rmpestano/cdi-crud[details here];
* CPU i7-2670QM
* 8GB RAM

=== Performance tests

As described earlier, this kind of test must be able to be executed frequently so it can catch changes in our code that _possible_ degrades the system performance.

It must not be destructive and IMO should execute on every significative change, for example it could be part of a http://martinfowler.com/bliki/DeploymentPipeline.html[continuous delivery pipeline].

NOTE: Jenkins has a https://wiki.jenkins-ci.org/display/JENKINS/Gatling+Plugin[Gatling plugin]).

In following sub-sections we are going to make changes to our code and infrastructure (Wildfly) and run the simulation to see if the change was good or not in terms of performance - mainly response time and throughput.

==== First execution

Here is the result of a execution without changes to the code:



TODO link reports online (eg flickr)

==== Adding server cache to REST endpoints

==== Asynchronous REST Responses

==== Wildfly on mode domain with load balance

==== HTTP2 enabled on wildfly

==== Using real database (eg postgress)

==== Wildfly Swarm
Here we are going to compare the *performance* of our application deployed on Wildfly (as we did till now) versus a fat-jar version of the same application orchestrated by Widlfly Swarm.

=== Load tests

without any improvement:
*mvn gatling:execute -Dtorture -Pperf*:
----
================================================================================
---- Global Information --------------------------------------------------------
> request count                                      90960 (OK=88985  KO=1975  )
> min response time                                      1 (OK=1      KO=4     )
> max response time                                    502 (OK=502    KO=26    )
> mean response time                                     7 (OK=7      KO=8     )
> std deviation                                          9 (OK=9      KO=1     )
> response time 50th percentile                          6 (OK=6      KO=8     )
> response time 75th percentile                          8 (OK=8      KO=9     )
> mean requests/sec                                 151.59 (OK=148.298 KO=3.291 )
---- Response Time Distribution ------------------------------------------------
> t < 800 ms                                         88985 ( 98%)
> 800 ms < t < 1200 ms                                   0 (  0%)
> t > 1200 ms                                            0 (  0%)
> failed                                              1975 (  2%)
---- Errors --------------------------------------------------------------------
> java.net.ConnectException: Cannot assign requested address       1975 (100.0%)
================================================================================

Reports generated in 4s.
Please open the following file: /home/pestano/projects/cdi-crud/target/gatling/results/CdiCrudRestSimulation-1431393875970/index.html
Global: percentage of successful requests is greater than 95 : true
Global: max of response time is less than 600 : true
Global: mean of response time is less than 15 : true
Global: mean requests per second is greater than 100 : true
----
Now that we have improved the application, its time to see how it behaves on very high traffic and see how much of load it supports.

For load/scalability tests we will take another approach. We will perform the simulation over a longer period (eg:30 min) and will increase users/requests slowly. At the same time we will attach a profiler (visualvm) and analyze resource consumption like memory, garbage collection, CPU usage, threads etc...


== References
. http://pt.slideshare.net/swapnilvkotwal/gatling
. https://mestachs.wordpress.com/2014/05/26/gatling-load-testing-like-a-king/
. https://developer.gooddata.com/how-we-do/performance-testing
. http://www.drdobbs.com/tools/continuous-integration-and-performance-t/206105978
. https://theholyjava.wordpress.com/wiki/development/testing/performance-testing-for-webapps-notes/
. http://devblog.orgsync.com/2013/05/03/rest-api-integration-testing-with-gatling/
. https://flood.io/blog/13-stress-testing-jmeter-and-gatling
. https://msdn.microsoft.com/en-us/library/bb924357.aspx
. https://gist.github.com/groovybayo/4691670
. https://github.com/gatling/gatling/blob/master/gatling-http/src/test/scala/io/gatling/http/HttpCompileTest.scala
. http://www.mastertheboss.com/jboss-server/wildfly-8/monitoring-wildfly-using-visualvm
. http://undertow.io/blog/2015/03/26/HTTP2-In-Wildfly.html
. https://docs.jboss.org/author/display/WFLY9/WildFly+9+Cluster+Howto
. https://www.youtube.com/watch?v=xa_gtRDpwyQ (intelligent load balance)
. https://docs.jboss.org/author/display/WFLY9/Admin+Guide